{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, RandomSampler,SequentialSampler\n",
    "\n",
    "from transformers import DistilBertTokenizer\n",
    "from transformers import DistilBertModel, DistilBertPreTrainedModel\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from scipy.special import logit\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA = (torch.cuda.device_count() > 0)\n",
    "MASK_IDX = 103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def platt_scale(outcome,probs):\n",
    "    logits = logit(probs)\n",
    "    logits = logits.reshape(-1,1)\n",
    "    log_reg = LogisticRegression(penalty='none', warm_start = True, solver = 'lbfgs' )\n",
    "    log_reg.fit(logits, outcome)\n",
    "    return log_reg.predict_proba(logits)\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1.0 + torch.erf(x/math.sqrt(2.0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bow_vector(ids, vocab_size, use_counts = False):\n",
    "    vec = torch.zeros(ids.shape[0],vocab_size)\n",
    "    ones = torch.ones_like(ids,dtype = torch.float)\n",
    "    if CUDA:\n",
    "        vec = vec.cuda()\n",
    "        ones = ones.cuda()\n",
    "        ids = ids.cuda()\n",
    "    vec.scatter_add_(1, ids,ones)\n",
    "    vec[:,1] = 0.0\n",
    "    if not use_counts:\n",
    "        vec = (vec != 0).float()\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalBert(DistilBertPreTrainedModel):\n",
    "    \"\"\"The model itself.\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.num_labels = config.num_labels\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        self.distilbert = DistilBertModel(config)\n",
    "        # self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.vocab_transform = nn.Linear(config.dim, config.dim)\n",
    "        self.vocab_layer_norm = nn.LayerNorm(config.dim, eps=1e-12)\n",
    "        self.vocab_projector = nn.Linear(config.dim, config.vocab_size)\n",
    "\n",
    "        self.Q_cls = nn.ModuleDict()\n",
    "\n",
    "        for T in range(2):\n",
    "            # ModuleDict keys have to be strings..\n",
    "            self.Q_cls['%d' % T] = nn.Sequential(\n",
    "                nn.Linear(config.hidden_size + self.num_labels, 200),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(200, self.num_labels))\n",
    "\n",
    "        self.g_cls = nn.Linear(config.hidden_size + self.num_labels, \n",
    "            self.config.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, W_ids, W_len, W_mask, C, T, Y=None, use_mlm=True):\n",
    "        if use_mlm:\n",
    "            W_len = W_len.unsqueeze(1) - 2 # -2 because of the +1 below\n",
    "            mask_class = torch.cuda.FloatTensor if CUDA else torch.FloatTensor\n",
    "            mask = (mask_class(W_len.shape).uniform_() * W_len.float()).long() + 1 # + 1 to avoid CLS\n",
    "            target_words = torch.gather(W_ids, 1, mask)\n",
    "            mlm_labels = torch.ones(W_ids.shape).long() * -100\n",
    "            if CUDA:\n",
    "                mlm_labels = mlm_labels.cuda()\n",
    "            mlm_labels.scatter_(1, mask, target_words)\n",
    "            W_ids.scatter_(1, mask, MASK_IDX)\n",
    "\n",
    "        outputs = self.distilbert(W_ids, attention_mask=W_mask)\n",
    "        seq_output = outputs[0]\n",
    "        pooled_output = seq_output[:, 0]\n",
    "        # seq_output, pooled_output = outputs[:2]\n",
    "        # pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "        if use_mlm:\n",
    "            prediction_logits = self.vocab_transform(seq_output)  # (bs, seq_length, dim)\n",
    "            prediction_logits = gelu(prediction_logits)  # (bs, seq_length, dim)\n",
    "            prediction_logits = self.vocab_layer_norm(prediction_logits)  # (bs, seq_length, dim)\n",
    "            prediction_logits = self.vocab_projector(prediction_logits)  # (bs, seq_length, vocab_size)\n",
    "            mlm_loss = CrossEntropyLoss()(\n",
    "                prediction_logits.view(-1, self.vocab_size), mlm_labels.view(-1))\n",
    "        else:\n",
    "            mlm_loss = 0.0\n",
    "\n",
    "        C_bow = make_bow_vector(C.unsqueeze(1), self.num_labels)\n",
    "        inputs = torch.cat((pooled_output, C_bow), 1)\n",
    "        \n",
    "        # g logits\n",
    "        g = self.g_cls(inputs)\n",
    "        \n",
    "        if Y is not None:  # TODO train/test mode, this is a lil hacky\n",
    "            g_loss = CrossEntropyLoss()(g.view(-1, self.num_labels), T.view(-1))\n",
    "        else:\n",
    "            g_loss = 0.0\n",
    "\n",
    "        # conditional expected outcome logits: \n",
    "        # run each example through its corresponding T matrix\n",
    "        # TODO this would be cleaner with sigmoid and BCELoss, but less general \n",
    "        #   (and I couldn't get it to work as well)\n",
    "        Q_logits_T0 = self.Q_cls['0'](inputs)\n",
    "        Q_logits_T1 = self.Q_cls['1'](inputs)\n",
    "\n",
    "        if Y is not None:\n",
    "            T0_indices = (T == 0).nonzero().squeeze()\n",
    "            Y_T1_labels = Y.clone().scatter(0, T0_indices, -100)\n",
    "\n",
    "            T1_indices = (T == 1).nonzero().squeeze()\n",
    "            Y_T0_labels = Y.clone().scatter(0, T1_indices, -100)\n",
    "\n",
    "            Q_loss_T1 = CrossEntropyLoss()(\n",
    "                Q_logits_T1.view(-1, self.num_labels), Y_T1_labels)\n",
    "            Q_loss_T0 = CrossEntropyLoss()(\n",
    "                Q_logits_T0.view(-1, self.num_labels), Y_T0_labels)\n",
    "\n",
    "            Q_loss = Q_loss_T0 + Q_loss_T1\n",
    "        else:\n",
    "            Q_loss = 0.0\n",
    "\n",
    "        sm = nn.Softmax(dim=1)\n",
    "        Q0 = sm(Q_logits_T0)[:, 1]\n",
    "        Q1 = sm(Q_logits_T1)[:, 1]\n",
    "        g = sm(g)[:, 1]\n",
    "\n",
    "        return g, Q0, Q1, g_loss, Q_loss, mlm_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalBertLightningModule(pl.LightningModule):\n",
    "    def __init__(self,g_weight = 1.0, Q_weight = 0.1, mlm_weight = 1.0, learning_rate = 2e-5,total_training_steps= None):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = CausalBert.from_pretrained(\n",
    "            \"distilbert-base-uncased\",\n",
    "            num_labels = 2,\n",
    "            output_attentions = False,\n",
    "            output_hidden_states = False\n",
    "        )\n",
    "\n",
    "        self.loss_weights = {\n",
    "            'g':g_weight,\n",
    "            'Q':Q_weight,\n",
    "            'mlm': mlm_weight\n",
    "        }\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.total_training_steps = total_training_steps\n",
    "    \n",
    "    def forward(self,W_ids, W_len, W_mask, C,T,Y = None, use_mlm = None):\n",
    "        return self.model(W_ids, W_len, W_mask,  C,T,Y, use_mlm)\n",
    "    \n",
    "    def training_step(self,batch,batch_idx):\n",
    "        W_ids,W_len,W_mask, C,T,Y = batch\n",
    "        g,Q0,Q1,g_loss,Q_loss,mlm_loss = self(W_ids, W_len,W_mask, C,T,Y)\n",
    "        loss = (self.loss_weights['g'] * g_loss + \n",
    "                self.loss_weights['Q'] * Q_loss + \n",
    "                self.loss_weights['mlm'] * mlm_loss)\n",
    "        self.log(\"train_loss\",loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        W_ids, W_len, W_mask, C,T,Y = batch\n",
    "        g, Q0, Q1, g_loss, Q_loss, _ = self(W_ids, W_len, W_mask, C, T, Y, use_mlm=False)\n",
    "        loss = self.loss_weights['g'] * g_loss + self.loss_weights['Q'] * Q_loss\n",
    "        self.log(\"val_loss\",loss)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def predict_step(self,batch,batch_idx):\n",
    "        W_ids, W_len, W_mask, C, T, Y = batch\n",
    "        g, Q0, Q1, _, _, _ = self(W_ids, W_len, W_mask, C, T, Y=None, use_mlm=False)\n",
    "        return Q0, Q1, Y\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr = self.learning_rate, eps = 1e-8)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=int(0.1 * self.total_training_steps),\n",
    "            num_training_steps=self.total_training_steps\n",
    "        )\n",
    "\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalBertDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,texts, confounds, treatments, outcomes, tokenizer, batch_size = 32):\n",
    "        super().__init__()\n",
    "        self.texts = texts\n",
    "        self.confounds = confounds\n",
    "        self.treatments = treatments\n",
    "        self.outcomes = outcomes\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "    def setup(self,stage = None):\n",
    "        inputs = self.tokenizer(\n",
    "            self.texts.tolist(),\n",
    "            add_special_tokens = True,\n",
    "            max_length = 128,\n",
    "            truncation = True,\n",
    "            padding = \"max_length\",\n",
    "            return_tensors = \"pt\"\n",
    "        )\n",
    "        W_len = inputs['attention_mask'].sum(dim=1)\n",
    "        dataset = TensorDataset(\n",
    "            inputs[\"input_ids\"],\n",
    "            W_len,\n",
    "            inputs[\"attention_mask\"],\n",
    "            torch.tensor(self.confounds.values),\n",
    "            torch.tensor(self.treatments.values),\n",
    "            torch.tensor(self.outcomes.values)\n",
    "        )\n",
    "        if stage == 'fit' or stage is None:\n",
    "            # データの分割（例：80%をトレーニング、20%を検証）\n",
    "            train_size = int(0.8 * len(dataset))\n",
    "            val_size = len(dataset) - train_size\n",
    "            self.train_dataset, self.val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "        if stage == 'predict' or stage is None:\n",
    "            self.predict_dataset = dataset  # 推論用データセッ\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size = self.batch_size, shuffle = True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size)\n",
    "    \n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.predict_dataset, batch_size=self.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../testdata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CausalBert were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['Q_cls.0.0.bias', 'Q_cls.0.0.weight', 'Q_cls.0.2.bias', 'Q_cls.0.2.weight', 'Q_cls.1.0.bias', 'Q_cls.1.0.weight', 'Q_cls.1.2.bias', 'Q_cls.1.2.weight', 'g_cls.bias', 'g_cls.weight', 'vocab_projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | CausalBert       | 90.7 M | eval \n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "90.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "90.7 M    Total params\n",
      "362.949   Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "106       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.venv/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.venv/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 461/461 [02:16<00:00,  3.38it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 461/461 [02:18<00:00,  3.33it/s, v_num=8]\n"
     ]
    }
   ],
   "source": [
    "data_module = CausalBertDataModule(\n",
    "    texts = df['text'],\n",
    "    confounds = df['C'],\n",
    "    treatments = df['T'],\n",
    "    outcomes = df['Y'],\n",
    "    tokenizer = tokenizer,\n",
    "    batch_size = 32\n",
    ")\n",
    "\n",
    "data_module.setup(stage = \"fit\")\n",
    "train_dataset_size = len(data_module.train_dataset)\n",
    "steps_per_epoch = train_dataset_size // data_module.batch_size\n",
    "total_training_steps = steps_per_epoch * 3  # max_epochs = 3\n",
    "\n",
    "model = CausalBertLightningModule(\n",
    "    g_weight = 0.1,\n",
    "    Q_weight = 0.1,\n",
    "    mlm_weight = 1.0,\n",
    "    learning_rate = 2e-5,\n",
    "    total_training_steps=total_training_steps\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs = 3,\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    devices=1 if torch.cuda.is_available() else None,\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "trainer.fit(model,data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/root/.venv/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 576/576 [01:06<00:00,  8.62it/s]\n",
      "Average Treatment Effect (ATE): 0.08383064717054367\n"
     ]
    }
   ],
   "source": [
    "# データモジュールのセットアップ\n",
    "data_module.setup(stage='predict')\n",
    "\n",
    "# 推論の実行\n",
    "predictions = trainer.predict(model, datamodule=data_module)\n",
    "\n",
    "# 予測結果の取得\n",
    "Q0s = []\n",
    "Q1s = []\n",
    "Ys = []\n",
    "\n",
    "for batch in predictions:\n",
    "    Q0_batch, Q1_batch, Y_batch = batch\n",
    "    Q0s.extend(Q0_batch.detach().cpu().numpy())\n",
    "    Q1s.extend(Q1_batch.detach().cpu().numpy())\n",
    "    Ys.extend(Y_batch.detach().cpu().numpy() if Y_batch is not None else [None]*len(Q0_batch))\n",
    "\n",
    "# ATEの計算\n",
    "Q0s = np.array(Q0s)\n",
    "Q1s = np.array(Q1s)\n",
    "ATE = np.mean(Q1s - Q0s)\n",
    "print(f\"Average Treatment Effect (ATE): {ATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>T</th>\n",
       "      <th>C</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>this is a great cd full of worship favorites!!...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>keith green had his special comedy style of ch...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>keith green was a true gift of god. his music ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>keith's music is a timeless message.  since hi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>if you're looking for a meditative, contemplat...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  T  C  Y\n",
       "0           0  this is a great cd full of worship favorites!!...  1  1  0\n",
       "1           1  keith green had his special comedy style of ch...  1  1  1\n",
       "2           2  keith green was a true gift of god. his music ...  1  0  0\n",
       "3           3  keith's music is a timeless message.  since hi...  1  0  0\n",
       "4           4  if you're looking for a meditative, contemplat...  1  0  1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
