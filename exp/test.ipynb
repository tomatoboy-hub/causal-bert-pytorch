{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys \n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shimizu/Desktop/Hayata/graduation_thetis/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config 770\n",
      "DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CausalBert were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['Q_cls.0.0.bias', 'Q_cls.0.0.weight', 'Q_cls.0.2.bias', 'Q_cls.0.2.weight', 'Q_cls.1.0.bias', 'Q_cls.1.0.weight', 'Q_cls.1.2.bias', 'Q_cls.1.2.weight', 'g_cls.bias', 'g_cls.weight', 'vocab_projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/shimizu/Desktop/Hayata/graduation_thetis/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/Users/shimizu/Desktop/Hayata/graduation_thetis/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/Users/shimizu/Desktop/Hayata/graduation_thetis/.venv/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/9202 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g torch.Size([2, 2]) tensor([[-0.0899,  0.0030],\n",
      "        [ 0.2262,  0.1941]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/9202 [00:00<1:04:54,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g torch.Size([2, 2]) tensor([[0.1583, 0.1952],\n",
      "        [0.2295, 0.1912]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/9202 [00:00<54:47,  2.80it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g torch.Size([2, 2]) tensor([[0.1791, 0.1131],\n",
      "        [0.0807, 0.2072]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/9202 [00:01<52:21,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g torch.Size([2, 2]) tensor([[0.1064, 0.1077],\n",
      "        [0.3027, 0.1123]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/9202 [00:01<50:35,  3.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g torch.Size([2, 2]) tensor([[0.1264, 0.1269],\n",
      "        [0.0504, 0.1276]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/9202 [00:01<49:34,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g torch.Size([2, 2]) tensor([[ 0.1300,  0.1817],\n",
      "        [-0.1627,  0.0421]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/9202 [00:01<49:08,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g torch.Size([2, 2]) tensor([[0.2198, 0.1539],\n",
      "        [0.1557, 0.1416]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/9202 [00:02<50:02,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g torch.Size([2, 2]) tensor([[0.1445, 0.1776],\n",
      "        [0.1210, 0.1690]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/9202 [00:02<49:11,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g torch.Size([2, 2]) tensor([[0.1136, 0.1129],\n",
      "        [0.1856, 0.0355]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/9202 [00:02<48:59,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g torch.Size([2, 2]) tensor([[ 0.1420,  0.1744],\n",
      "        [-0.0106,  0.1156]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/9202 [00:03<51:09,  2.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g torch.Size([2, 2]) tensor([[-0.0608, -0.0671],\n",
      "        [ 0.1064,  0.2858]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/9202 [00:03<50:51,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g torch.Size([2, 2]) tensor([[-0.0647,  0.1808],\n",
      "        [-0.0916,  0.2501]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/9202 [00:03<50:24,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g torch.Size([2, 2]) tensor([[0.0026, 0.0649],\n",
      "        [0.2354, 0.1579]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 13/9202 [00:04<51:45,  2.96it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../testdata.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m cb \u001b[38;5;241m=\u001b[39m CausalBertWrapper(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, g_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, Q_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,mlm_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mcb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(cb\u001b[38;5;241m.\u001b[39mATE(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m],df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m],platt_scaling\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "File \u001b[0;32m~/Desktop/Hayata/graduation_thetis/causal-bert-pytorch/exp/../CausalBert_base.py:220\u001b[0m, in \u001b[0;36mCausalBertWrapper.train\u001b[0;34m(self, texts, confounds, treatments, outcomes, learning_rate, epochs)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# while True:\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 220\u001b[0m g, Q0, Q1, g_loss, Q_loss, mlm_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mg\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m g_loss \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQ\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m Q_loss \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlm\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m mlm_loss\n\u001b[1;32m    224\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Desktop/Hayata/graduation_thetis/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Hayata/graduation_thetis/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Hayata/graduation_thetis/causal-bert-pytorch/exp/../CausalBert_base.py:128\u001b[0m, in \u001b[0;36mCausalBert.forward\u001b[0;34m(self, W_ids, W_len, W_mask, C, T, Y, use_mlm)\u001b[0m\n\u001b[1;32m    126\u001b[0m     prediction_logits \u001b[38;5;241m=\u001b[39m gelu(prediction_logits)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     prediction_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_layer_norm(prediction_logits)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m     prediction_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_projector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction_logits\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (bs, seq_length, vocab_size)\u001b[39;00m\n\u001b[1;32m    129\u001b[0m     mlm_loss \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()(\n\u001b[1;32m    130\u001b[0m         prediction_logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size), mlm_labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/Hayata/graduation_thetis/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Hayata/graduation_thetis/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Hayata/graduation_thetis/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from CausalBert_base import CausalBertWrapper\n",
    "df = pd.read_csv(\"../testdata.csv\")\n",
    "\n",
    "cb = CausalBertWrapper(batch_size=2, g_weight=0.1, Q_weight=0.1,mlm_weight=1)\n",
    "cb.train(df[\"text\"], df[\"C\"], df[\"T\"], df[\"Y\"], epochs = 1)\n",
    "print(cb.ATE(df['C'],df['text'],platt_scaling=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "3074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CausalBert were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.Q_cls.0.0.bias', 'roberta.Q_cls.0.0.weight', 'roberta.Q_cls.0.2.bias', 'roberta.Q_cls.0.2.weight', 'roberta.Q_cls.1.0.bias', 'roberta.Q_cls.1.0.weight', 'roberta.Q_cls.1.2.bias', 'roberta.Q_cls.1.2.weight', 'roberta.distilbert.embeddings.LayerNorm.bias', 'roberta.distilbert.embeddings.LayerNorm.weight', 'roberta.distilbert.embeddings.position_embeddings.weight', 'roberta.distilbert.embeddings.token_type_embeddings.weight', 'roberta.distilbert.embeddings.word_embeddings.weight', 'roberta.distilbert.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.distilbert.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.distilbert.encoder.layer.0.attention.output.dense.bias', 'roberta.distilbert.encoder.layer.0.attention.output.dense.weight', 'roberta.distilbert.encoder.layer.0.attention.self.key.bias', 'roberta.distilbert.encoder.layer.0.attention.self.key.weight', 'roberta.distilbert.encoder.layer.0.attention.self.query.bias', 'roberta.distilbert.encoder.layer.0.attention.self.query.weight', 'roberta.distilbert.encoder.layer.0.attention.self.value.bias', 'roberta.distilbert.encoder.layer.0.attention.self.value.weight', 'roberta.distilbert.encoder.layer.0.intermediate.dense.bias', 'roberta.distilbert.encoder.layer.0.intermediate.dense.weight', 'roberta.distilbert.encoder.layer.0.output.LayerNorm.bias', 'roberta.distilbert.encoder.layer.0.output.LayerNorm.weight', 'roberta.distilbert.encoder.layer.0.output.dense.bias', 'roberta.distilbert.encoder.layer.0.output.dense.weight', 'roberta.distilbert.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.distilbert.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.distilbert.encoder.layer.1.attention.output.dense.bias', 'roberta.distilbert.encoder.layer.1.attention.output.dense.weight', 'roberta.distilbert.encoder.layer.1.attention.self.key.bias', 'roberta.distilbert.encoder.layer.1.attention.self.key.weight', 'roberta.distilbert.encoder.layer.1.attention.self.query.bias', 'roberta.distilbert.encoder.layer.1.attention.self.query.weight', 'roberta.distilbert.encoder.layer.1.attention.self.value.bias', 'roberta.distilbert.encoder.layer.1.attention.self.value.weight', 'roberta.distilbert.encoder.layer.1.intermediate.dense.bias', 'roberta.distilbert.encoder.layer.1.intermediate.dense.weight', 'roberta.distilbert.encoder.layer.1.output.LayerNorm.bias', 'roberta.distilbert.encoder.layer.1.output.LayerNorm.weight', 'roberta.distilbert.encoder.layer.1.output.dense.bias', 'roberta.distilbert.encoder.layer.1.output.dense.weight', 'roberta.distilbert.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.distilbert.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.distilbert.encoder.layer.10.attention.output.dense.bias', 'roberta.distilbert.encoder.layer.10.attention.output.dense.weight', 'roberta.distilbert.encoder.layer.10.attention.self.key.bias', 'roberta.distilbert.encoder.layer.10.attention.self.key.weight', 'roberta.distilbert.encoder.layer.10.attention.self.query.bias', 'roberta.distilbert.encoder.layer.10.attention.self.query.weight', 'roberta.distilbert.encoder.layer.10.attention.self.value.bias', 'roberta.distilbert.encoder.layer.10.attention.self.value.weight', 'roberta.distilbert.encoder.layer.10.intermediate.dense.bias', 'roberta.distilbert.encoder.layer.10.intermediate.dense.weight', 'roberta.distilbert.encoder.layer.10.output.LayerNorm.bias', 'roberta.distilbert.encoder.layer.10.output.LayerNorm.weight', 'roberta.distilbert.encoder.layer.10.output.dense.bias', 'roberta.distilbert.encoder.layer.10.output.dense.weight', 'roberta.distilbert.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.distilbert.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.distilbert.encoder.layer.11.attention.output.dense.bias', 'roberta.distilbert.encoder.layer.11.attention.output.dense.weight', 'roberta.distilbert.encoder.layer.11.attention.self.key.bias', 'roberta.distilbert.encoder.layer.11.attention.self.key.weight', 'roberta.distilbert.encoder.layer.11.attention.self.query.bias', 'roberta.distilbert.encoder.layer.11.attention.self.query.weight', 'roberta.distilbert.encoder.layer.11.attention.self.value.bias', 'roberta.distilbert.encoder.layer.11.attention.self.value.weight', 'roberta.distilbert.encoder.layer.11.intermediate.dense.bias', 'roberta.distilbert.encoder.layer.11.intermediate.dense.weight', 'roberta.distilbert.encoder.layer.11.output.LayerNorm.bias', 'roberta.distilbert.encoder.layer.11.output.LayerNorm.weight', 'roberta.distilbert.encoder.layer.11.output.dense.bias', 'roberta.distilbert.encoder.layer.11.output.dense.weight', 'roberta.distilbert.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.distilbert.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.distilbert.encoder.layer.2.attention.output.dense.bias', 'roberta.distilbert.encoder.layer.2.attention.output.dense.weight', 'roberta.distilbert.encoder.layer.2.attention.self.key.bias', 'roberta.distilbert.encoder.layer.2.attention.self.key.weight', 'roberta.distilbert.encoder.layer.2.attention.self.query.bias', 'roberta.distilbert.encoder.layer.2.attention.self.query.weight', 'roberta.distilbert.encoder.layer.2.attention.self.value.bias', 'roberta.distilbert.encoder.layer.2.attention.self.value.weight', 'roberta.distilbert.encoder.layer.2.intermediate.dense.bias', 'roberta.distilbert.encoder.layer.2.intermediate.dense.weight', 'roberta.distilbert.encoder.layer.2.output.LayerNorm.bias', 'roberta.distilbert.encoder.layer.2.output.LayerNorm.weight', 'roberta.distilbert.encoder.layer.2.output.dense.bias', 'roberta.distilbert.encoder.layer.2.output.dense.weight', 'roberta.distilbert.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.distilbert.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.distilbert.encoder.layer.3.attention.output.dense.bias', 'roberta.distilbert.encoder.layer.3.attention.output.dense.weight', 'roberta.distilbert.encoder.layer.3.attention.self.key.bias', 'roberta.distilbert.encoder.layer.3.attention.self.key.weight', 'roberta.distilbert.encoder.layer.3.attention.self.query.bias', 'roberta.distilbert.encoder.layer.3.attention.self.query.weight', 'roberta.distilbert.encoder.layer.3.attention.self.value.bias', 'roberta.distilbert.encoder.layer.3.attention.self.value.weight', 'roberta.distilbert.encoder.layer.3.intermediate.dense.bias', 'roberta.distilbert.encoder.layer.3.intermediate.dense.weight', 'roberta.distilbert.encoder.layer.3.output.LayerNorm.bias', 'roberta.distilbert.encoder.layer.3.output.LayerNorm.weight', 'roberta.distilbert.encoder.layer.3.output.dense.bias', 'roberta.distilbert.encoder.layer.3.output.dense.weight', 'roberta.distilbert.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.distilbert.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.distilbert.encoder.layer.4.attention.output.dense.bias', 'roberta.distilbert.encoder.layer.4.attention.output.dense.weight', 'roberta.distilbert.encoder.layer.4.attention.self.key.bias', 'roberta.distilbert.encoder.layer.4.attention.self.key.weight', 'roberta.distilbert.encoder.layer.4.attention.self.query.bias', 'roberta.distilbert.encoder.layer.4.attention.self.query.weight', 'roberta.distilbert.encoder.layer.4.attention.self.value.bias', 'roberta.distilbert.encoder.layer.4.attention.self.value.weight', 'roberta.distilbert.encoder.layer.4.intermediate.dense.bias', 'roberta.distilbert.encoder.layer.4.intermediate.dense.weight', 'roberta.distilbert.encoder.layer.4.output.LayerNorm.bias', 'roberta.distilbert.encoder.layer.4.output.LayerNorm.weight', 'roberta.distilbert.encoder.layer.4.output.dense.bias', 'roberta.distilbert.encoder.layer.4.output.dense.weight', 'roberta.distilbert.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.distilbert.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.distilbert.encoder.layer.5.attention.output.dense.bias', 'roberta.distilbert.encoder.layer.5.attention.output.dense.weight', 'roberta.distilbert.encoder.layer.5.attention.self.key.bias', 'roberta.distilbert.encoder.layer.5.attention.self.key.weight', 'roberta.distilbert.encoder.layer.5.attention.self.query.bias', 'roberta.distilbert.encoder.layer.5.attention.self.query.weight', 'roberta.distilbert.encoder.layer.5.attention.self.value.bias', 'roberta.distilbert.encoder.layer.5.attention.self.value.weight', 'roberta.distilbert.encoder.layer.5.intermediate.dense.bias', 'roberta.distilbert.encoder.layer.5.intermediate.dense.weight', 'roberta.distilbert.encoder.layer.5.output.LayerNorm.bias', 'roberta.distilbert.encoder.layer.5.output.LayerNorm.weight', 'roberta.distilbert.encoder.layer.5.output.dense.bias', 'roberta.distilbert.encoder.layer.5.output.dense.weight', 'roberta.distilbert.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.distilbert.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.distilbert.encoder.layer.6.attention.output.dense.bias', 'roberta.distilbert.encoder.layer.6.attention.output.dense.weight', 'roberta.distilbert.encoder.layer.6.attention.self.key.bias', 'roberta.distilbert.encoder.layer.6.attention.self.key.weight', 'roberta.distilbert.encoder.layer.6.attention.self.query.bias', 'roberta.distilbert.encoder.layer.6.attention.self.query.weight', 'roberta.distilbert.encoder.layer.6.attention.self.value.bias', 'roberta.distilbert.encoder.layer.6.attention.self.value.weight', 'roberta.distilbert.encoder.layer.6.intermediate.dense.bias', 'roberta.distilbert.encoder.layer.6.intermediate.dense.weight', 'roberta.distilbert.encoder.layer.6.output.LayerNorm.bias', 'roberta.distilbert.encoder.layer.6.output.LayerNorm.weight', 'roberta.distilbert.encoder.layer.6.output.dense.bias', 'roberta.distilbert.encoder.layer.6.output.dense.weight', 'roberta.distilbert.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.distilbert.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.distilbert.encoder.layer.7.attention.output.dense.bias', 'roberta.distilbert.encoder.layer.7.attention.output.dense.weight', 'roberta.distilbert.encoder.layer.7.attention.self.key.bias', 'roberta.distilbert.encoder.layer.7.attention.self.key.weight', 'roberta.distilbert.encoder.layer.7.attention.self.query.bias', 'roberta.distilbert.encoder.layer.7.attention.self.query.weight', 'roberta.distilbert.encoder.layer.7.attention.self.value.bias', 'roberta.distilbert.encoder.layer.7.attention.self.value.weight', 'roberta.distilbert.encoder.layer.7.intermediate.dense.bias', 'roberta.distilbert.encoder.layer.7.intermediate.dense.weight', 'roberta.distilbert.encoder.layer.7.output.LayerNorm.bias', 'roberta.distilbert.encoder.layer.7.output.LayerNorm.weight', 'roberta.distilbert.encoder.layer.7.output.dense.bias', 'roberta.distilbert.encoder.layer.7.output.dense.weight', 'roberta.distilbert.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.distilbert.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.distilbert.encoder.layer.8.attention.output.dense.bias', 'roberta.distilbert.encoder.layer.8.attention.output.dense.weight', 'roberta.distilbert.encoder.layer.8.attention.self.key.bias', 'roberta.distilbert.encoder.layer.8.attention.self.key.weight', 'roberta.distilbert.encoder.layer.8.attention.self.query.bias', 'roberta.distilbert.encoder.layer.8.attention.self.query.weight', 'roberta.distilbert.encoder.layer.8.attention.self.value.bias', 'roberta.distilbert.encoder.layer.8.attention.self.value.weight', 'roberta.distilbert.encoder.layer.8.intermediate.dense.bias', 'roberta.distilbert.encoder.layer.8.intermediate.dense.weight', 'roberta.distilbert.encoder.layer.8.output.LayerNorm.bias', 'roberta.distilbert.encoder.layer.8.output.LayerNorm.weight', 'roberta.distilbert.encoder.layer.8.output.dense.bias', 'roberta.distilbert.encoder.layer.8.output.dense.weight', 'roberta.distilbert.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.distilbert.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.distilbert.encoder.layer.9.attention.output.dense.bias', 'roberta.distilbert.encoder.layer.9.attention.output.dense.weight', 'roberta.distilbert.encoder.layer.9.attention.self.key.bias', 'roberta.distilbert.encoder.layer.9.attention.self.key.weight', 'roberta.distilbert.encoder.layer.9.attention.self.query.bias', 'roberta.distilbert.encoder.layer.9.attention.self.query.weight', 'roberta.distilbert.encoder.layer.9.attention.self.value.bias', 'roberta.distilbert.encoder.layer.9.attention.self.value.weight', 'roberta.distilbert.encoder.layer.9.intermediate.dense.bias', 'roberta.distilbert.encoder.layer.9.intermediate.dense.weight', 'roberta.distilbert.encoder.layer.9.output.LayerNorm.bias', 'roberta.distilbert.encoder.layer.9.output.LayerNorm.weight', 'roberta.distilbert.encoder.layer.9.output.dense.bias', 'roberta.distilbert.encoder.layer.9.output.dense.weight', 'roberta.distilbert.pooler.dense.bias', 'roberta.distilbert.pooler.dense.weight', 'roberta.g_cls.bias', 'roberta.g_cls.weight', 'roberta.vocab_layer_norm.bias', 'roberta.vocab_layer_norm.weight', 'roberta.vocab_projector.bias', 'roberta.vocab_projector.weight', 'roberta.vocab_transform.bias', 'roberta.vocab_transform.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/shimizu/Desktop/Hayata/graduation_thetis/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/Users/shimizu/Desktop/Hayata/graduation_thetis/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/Users/shimizu/Desktop/Hayata/graduation_thetis/.venv/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/9202 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 1.2123, -0.0637,  0.7337,  ...,  0.1737,  0.6710,  0.8518],\n",
      "         [ 1.0416,  0.7900,  0.0895,  ...,  0.7434,  0.4747,  1.0562],\n",
      "         [ 1.5457,  0.5516,  0.0206,  ...,  0.9420,  0.5657,  0.9774],\n",
      "         ...,\n",
      "         [ 1.2356,  0.5224,  0.5104,  ...,  0.9015,  1.0861, -0.6498],\n",
      "         [ 1.5612,  1.1832,  0.0849,  ...,  1.7272,  1.6004, -0.5038],\n",
      "         [ 1.0681,  0.7594,  0.2545,  ...,  0.7994,  1.5101, -0.6774]],\n",
      "\n",
      "        [[ 1.1104,  0.5254, -0.4966,  ...,  0.7713,  1.6232,  0.3436],\n",
      "         [ 0.6488,  0.7971, -0.3156,  ...,  1.5227,  1.4837, -0.4034],\n",
      "         [ 1.5540,  0.8937, -0.0946,  ...,  1.8694,  0.7878,  0.9292],\n",
      "         ...,\n",
      "         [ 0.7367,  1.1583, -0.1198,  ...,  0.9236,  1.0798,  0.3323],\n",
      "         [ 0.6798,  0.7158, -0.5693,  ...,  0.8191,  0.4552, -0.0934],\n",
      "         [ 1.5657,  0.0828, -0.5274,  ...,  1.5784,  1.3162,  0.3986]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 0.2476,  0.5129, -0.0229,  ...,  0.0247, -0.6187,  0.4824],\n",
      "        [-0.4101,  0.2822, -0.4885,  ...,  0.0575,  0.2101,  0.7372]],\n",
      "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n",
      "seq_output shape: torch.Size([2, 128, 768])\n",
      "pooled_output shape: torch.Size([2, 768])\n",
      "vocab_transform output shape: torch.Size([2, 128, 768])\n",
      "torch.Size([2, 128, 768])\n",
      "torch.Size([2, 128, 768])\n",
      "torch.Size([2, 128, 768])\n",
      "torch.Size([2, 128, 50265])\n",
      "tensor(11.2728, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x770 and 3074x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../testdata.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m cb \u001b[38;5;241m=\u001b[39m CausalBertWrapper(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, g_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, Q_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,mlm_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mcb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(cb\u001b[38;5;241m.\u001b[39mATE(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m],df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m],platt_scaling\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "File \u001b[0;32m~/Desktop/Hayata/graduation_thetis/causal-bert-pytorch/exp/../CausalBert.py:228\u001b[0m, in \u001b[0;36mCausalBertWrapper.train\u001b[0;34m(self, texts, confounds, treatments, outcomes, learning_rate, epochs)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# while True:\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 228\u001b[0m g, Q0, Q1, g_loss, Q_loss, mlm_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mg\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m g_loss \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQ\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m Q_loss \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlm\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m mlm_loss\n\u001b[1;32m    232\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Desktop/Hayata/graduation_thetis/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Hayata/graduation_thetis/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Hayata/graduation_thetis/causal-bert-pytorch/exp/../CausalBert.py:147\u001b[0m, in \u001b[0;36mCausalBert.forward\u001b[0;34m(self, W_ids, W_len, W_mask, C, T, Y, use_mlm)\u001b[0m\n\u001b[1;32m    144\u001b[0m inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((pooled_output, C_bow), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# g logits\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m g \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mg_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# TODO train/test mode, this is a lil hacky\u001b[39;00m\n\u001b[1;32m    149\u001b[0m     g_loss \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()(g\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_labels), T\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/Desktop/Hayata/graduation_thetis/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Hayata/graduation_thetis/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Hayata/graduation_thetis/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x770 and 3074x2)"
     ]
    }
   ],
   "source": [
    "from CausalBert import CausalBertWrapper\n",
    "df = pd.read_csv(\"../testdata.csv\")\n",
    "\n",
    "cb = CausalBertWrapper(batch_size=2, g_weight=0.1, Q_weight=0.1,mlm_weight=1)\n",
    "cb.train(df[\"text\"], df[\"C\"], df[\"T\"], df[\"Y\"], epochs = 1)\n",
    "print(cb.ATE(df['C'],df['text'],platt_scaling=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
